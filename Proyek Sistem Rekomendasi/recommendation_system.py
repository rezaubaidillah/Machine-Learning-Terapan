# -*- coding: utf-8 -*-
"""recommendation-system.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KU7QOZs8n3GpExuS3F2MJzQo-FE4SkHe

# By RezaUbaidillah
[Github](https://github.com/rezaubaidillah/Machine-Learning-Terapan/tree/main/Proyek%20Sistem%20Rekomendasi)

Terdapat 3 File CSV
- events1.csv
- items.csv
- users.csv

## Data Gathering

Mendownload data
"""

!kaggle datasets download mexwell/google-merchandise-sales-data

"""Unziping data"""

!unzip google-merchandise-sales-data.zip

"""## Data Understanding

### Import Liblaries yang Diperlukan
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

transaksi = pd.read_csv('/content/events1.csv')
barang = pd.read_csv('/content/items.csv')
profil = pd.read_csv('/content/users.csv')

print('Jumlah data event yang diterima: ', len(transaksi))
print('Jumlah data barang: ', len(barang.id.unique()))
print('Jumlah data profil: ', len(profil.id.unique()))

"""### Univariate Exploratory Data Analysis

##### dataframe transaksi
"""

transaksi.info()

transaksi.head()

"""Kita akan mengambil user_id, item_id dan country saja sisanya kita hapus"""

print('Banyak data: ', len(transaksi))
print('Jumlah user_id: ', len(transaksi.user_id.unique()))
print('Jenis ga_session_id : ', transaksi.ga_session_id.unique())
print('Jenis country : ', transaksi.country.unique())
print('Jenis device: ', transaksi.device.unique())
print('jenis item_id: ', len(transaksi.item_id.unique()))
print('jenis date: ', len(transaksi.date.unique()))

"""##### dataframe barang"""

barang.info()

barang.head()

print('Banyak barang: ', len(barang.id.unique()))
print('Jenis brand : ', barang.brand.unique())
print('Jenis variant : ', barang.variant.unique())
print('Jenis category : ', barang.category.unique())

"""#### Dataframe profil"""

profil.info()

profil.head()

print('Banyak id: ', len(profil.id.unique()))
print('banyak ltv : ', len(profil.ltv.unique()))
print('banyak date : ', len(profil.date.unique()))

"""#### EDA

membuat dataframe untuk eda
"""

# Merge data transaksi dengan data barang untuk mendapatkan informasi kategori produk
eda_etransaksi = transaksi.merge(barang[['id', 'category']], left_on='item_id', right_on='id', how='left')

# Set style untuk seaborn
sns.set(style="whitegrid")

"""menampilkan  Distribusi category Berdasarkan Transaksi"""

# Plot distribusi kategori produk
plt.figure(figsize=(12, 6))
sns.countplot(data=eda_etransaksi, y='category', order=eda_etransaksi['category'].value_counts().index, palette="viridis")
plt.title('Distribusi Kategori Berdasarkan Transaksi')
plt.xlabel('Jumlah Transaksi')
plt.ylabel('Kategori Produk')
plt.show()

item_counts = eda_etransaksi['item_id'].value_counts()

# Ambil 10 item_id dengan frekuensi tertinggi
top_10_items = item_counts.head(10)

# Buat plot
sns.countplot(data=eda_etransaksi, y='item_id', order=top_10_items.index, palette="viridis")

# Tambahkan judul dan label sumbu (opsional)
plt.title('10 Item ID dengan Frekuensi Transaksi Tertinggi')
plt.xlabel('Jumlah Transaksi')
plt.ylabel('Item ID')

# Hitung frekuensi kemunculan setiap user_id
user_counts = eda_etransaksi['user_id'].value_counts()

# Ambil 10 user_id dengan frekuensi tertinggi
top_10_users = user_counts.head(10)

# Buat plot
sns.countplot(data=eda_etransaksi, y='user_id', order=top_10_users.index, palette="viridis")

# Tambahkan judul dan label sumbu (opsional)
plt.title('10 user ID dengan Frekuensi Transaksi Tertinggi')
plt.xlabel('Jumlah Transaksi')
plt.ylabel('user ID')

# Tampilkan plot
plt.show()

"""## Data Preprocessing

### Mengubah kolom id pada dataframe barang menjadi item_id
"""

barang = barang.rename(columns={'id': 'item_id'})

"""### Mengubah kolom id pada dataframe profil menjadi user_id"""

barang = barang.rename(columns={'id': 'user_id'})

"""### Menggabungkan Data dengan Fitur Nama barang"""

all_transaksi_name = pd.merge(transaksi, barang[['item_id','name','brand','variant','category','price_in_usd']], on='item_id', how='left')

# Print dataframe all_transaksi_name
all_transaksi_name

"""## Data Preparation

### Mengatasi Missing Value
"""

all_transaksi_name.isnull().sum()

"""karena hampir semua kolom variant memiliki nilai null dan kita tidak menggunakan kolom variant kita drop saja, kita hanya menghapus country"""

all_transaksi_name_clean = all_transaksi_name.dropna(subset=['country'])

"""drop kolom variant"""

all_transaksi_name_clean = all_transaksi_name_clean.drop('variant', axis=1)

"""### Mengatasi Duplicated Value"""

all_transaksi_name_clean[all_transaksi_name_clean.name.eq('Android Hipster Pin')]

"""pada barang yang memiliki nama Android Hipster Pin memiliki item id yang berbeda"""

all_transaksi_name_clean

"""mengubah item_id menjadi item id terkecil yang memiliki nama yang sama"""

# Step 1: Temukan item_id terkecil untuk setiap name
all_transaksi_name_clean['item_id_min'] = all_transaksi_name_clean.groupby('name')['item_id'].transform('min')

# Step 2: Ganti semua item_id yang berbeda dengan item_id_min untuk setiap name
all_transaksi_name_clean['item_id'] = all_transaksi_name_clean['item_id_min']

# Step 3: Hapus kolom item_id_min (optional)
all_transaksi_name_clean.drop(columns=['item_id_min'], inplace=True)

# Output hasil
print(all_transaksi_name_clean)

"""### Menyamakan Jenis nama barang item_id"""

fix_transaksi = all_transaksi_name_clean.sort_values('item_id', ascending=True)
print(fix_transaksi)

len(fix_transaksi.item_id.unique())

fix_transaksi.category.unique()

preparation = fix_transaksi
preparation.sort_values('item_id')

preparation = preparation.drop_duplicates('item_id')
preparation

"""### melakukan konversi data series menjadi list."""

# Mengonversi data series ‘item_id’ menjadi dalam bentuk list
barang_id = preparation['item_id'].tolist()

# Mengonversi data series ‘Name’ menjadi dalam bentuk list
nama_barang = preparation['name'].tolist()

# Mengonversi data series ‘category’ menjadi dalam bentuk list
category_barang = preparation['category'].tolist()
print(len(barang_id))
print(len(nama_barang))
print(len(category_barang))

"""membuat dictionary untuk menentukan pasangan key-value pada data barang_id, nama_barang, dan category_barang yang telah kita siapkan sebelumnya."""

transaksi_new = pd.DataFrame({
    'item_id': barang_id,
    'nama_barang': nama_barang,
    'category': category_barang
})
transaksi_new

"""## Model Development dengan Content Based Filtering

### TF-IDF Vectorizer
"""

data = transaksi_new
data.sample(5)

from sklearn.feature_extraction.text import TfidfVectorizer

# Inisialisasi TfidfVectorizer
tf = TfidfVectorizer()

# Melakukan perhitungan idf pada data category
tf.fit(data['category'])

# Mapping array dari fitur index integer ke fitur nama
tf.get_feature_names_out()

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tf.fit_transform(data['category'])

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

tfidf_matrix.todense()

# Membuat dataframe untuk melihat tf-idf matrix
# Kolom diisi dengan jenis masakan
# Baris diisi dengan nama barang

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=data.nama_barang
).sample(32, axis=1).sample(10, axis=0)

"""###Cosine Similarity"""

from sklearn.metrics.pairwise import cosine_similarity

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa nama barang
cosine_sim_df = pd.DataFrame(cosine_sim, index=data['nama_barang'], columns=data['nama_barang'])
print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix pada setiap item
cosine_sim_df.sample(10, axis=1).sample(50, axis=0)

"""### Mendapatkan Rekomendasi"""

def barang_recommendations(input_nama_barang, similarity_data=cosine_sim_df, items=data[['nama_barang', 'category']], k=20):
    """
    Rekomendasi barang berdasarkan kemiripan dataframe

    Parameter:
    ---
    input_nama_barang : tipe data string (str)
                Nama barang (index kemiripan dataframe)
    similarity_data : tipe data pd.DataFrame (object)
                      Kesamaan dataframe, simetrik, dengan barang sebagai
                      indeks dan kolom
    items : tipe data pd.DataFrame (object)
            Mengandung kedua nama dan fitur lainnya yang digunakan untuk mendefinisikan kemiripan
    k : tipe data integer (int)
        Banyaknya jumlah rekomendasi yang diberikan
    ---


    Pada index ini, kita mengambil k dengan nilai similarity terbesar
    pada index matrix yang diberikan (i).
    """


    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,input_nama_barang].to_numpy().argpartition(
        range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop input_nama_barang agar nama barang yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(input_nama_barang, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

data[data.nama_barang.eq('Google Flat Front Bag Grey')]

barang_recommendations('Google Flat Front Bag Grey')

"""## Model Development dengan Collaborative Filtering

### Data Preparation
"""

all_transaksi_name_clean

"""karena pada dataset kita tidak memilki rating, kita akan menggunakan type sebagai gantinya jika user
- add_to_cart = 1
- begin_checkout = 2
- purchase = 3
"""

# Create a mapping for event types
event_type_mapping = {
    'add_to_cart': 1,
    'begin_checkout': 2,
    'purchase': 3
}

# Convert the 'event' column to numerical values based on the mapping
all_transaksi_name_clean['event_type'] = all_transaksi_name_clean['type'].map(event_type_mapping)

# Print some info to check the changes
print(all_transaksi_name_clean['event_type'].value_counts())
all_transaksi_name_clean.head()

df = all_transaksi_name_clean[['user_id', 'item_id', 'event_type']]

# Mengubah user_id menjadi list tanpa nilai yang sama
user_ids = df['user_id'].unique().tolist()
print('list user_id: ', user_ids)

# Melakukan encoding user_id
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded user_id : ', user_to_user_encoded)

# Melakukan proses encoding angka ke ke user_id
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke user_id: ', user_encoded_to_user)

# Mengubah item_id menjadi list tanpa nilai yang sama
item_ids = df['item_id'].unique().tolist()

# Melakukan proses encoding item_id
item_to_item_encoded = {x: i for i, x in enumerate(item_ids)}
print('encoded item_id : ', item_to_item_encoded)
# Melakukan proses encoding angka ke item_id
item_encoded_to_item = {i: x for i, x in enumerate(item_ids)}
print('encoded angka ke item_id: ', item_encoded_to_item)

# Mapping user_id ke dataframe user
df['user'] = df['user_id'].map(user_to_user_encoded)

# Mapping item_id ke dataframe item
df['item'] = df['item_id'].map(item_to_item_encoded)

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

# Mendapatkan jumlah item
num_item = len(item_encoded_to_item)
print(num_item)

# Mengubah event_type menjadi nilai float
df['event_type'] = df['event_type'].values.astype(np.int32)

# Nilai minimum event_type
min_event_type = min(df['event_type'])

# Nilai maksimal event_type
max_event_type = max(df['event_type'])

print('Number of User: {}, Number of item: {}, Min event_type: {}, Max event_type: {}'.format(
    num_users, num_item, min_event_type, max_event_type
))

"""#### Membagi Data untuk Training dan Validasi"""

df = df.sample(frac=1, random_state=42)
df

# Membuat variabel x untuk mencocokkan data user dan item menjadi satu value
x = df[['user', 'item']].values

# Membuat variabel y untuk membuat event_type dari hasil
y = df['event_type'].apply(lambda x: (x - min_event_type) / (max_event_type - min_event_type)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)
print(x, y)

"""### Proses Training

"""

import tensorflow as tf # Import tensorflow
from tensorflow import keras
from tensorflow.keras import layers

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_item, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_item = num_item
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.item_embedding = layers.Embedding( # layer embeddings item
        num_item,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.item_bias = layers.Embedding(num_item, 1) # layer embedding item bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    item_vector = self.item_embedding(inputs[:, 1]) # memanggil layer embedding 3
    item_bias = self.item_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_item = tf.tensordot(user_vector, item_vector, 2)

    x = dot_user_item + user_bias + item_bias

    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_users, num_item, 50) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 64,
    epochs = 50,
    validation_data = (x_val, y_val)
)

"""### Visualisasi Metrik"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""### Mendapatkan Rekomendasi Item"""

item_df = all_transaksi_name_clean[['item_id','name','category']]
df = all_transaksi_name_clean[['user_id', 'item_id', 'event_type']]

# Mengambil sample user
user_id = df.user_id.sample(1).iloc[0]
item_visited_by_user = df[df.user_id == user_id]

# Operator bitwise (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html
item_not_visited = item_df[~item_df['item_id'].isin(item_visited_by_user.item_id.values)]['item_id']
item_not_visited = list(
    set(item_not_visited)
    .intersection(set(item_to_item_encoded.keys()))
)

item_not_visited = [[item_to_item_encoded.get(x)] for x in item_not_visited]
user_encoder = user_to_user_encoded.get(user_id)
user_item_array = np.hstack(
    ([[user_encoder]] * len(item_not_visited), item_not_visited)
)

# Predict ratings
ratings = model.predict(user_item_array).flatten()

# Get top 10 indices of highest ratings
top_ratings_indices = ratings.argsort()[-10:][::-1]

# Ensure recommended item IDs are unique
recommended_item_ids = list(set([
    item_encoded_to_item.get(item_not_visited[x][0]) for x in top_ratings_indices
]))

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)

# Limit to only showing top 10 items with the highest event_type from the user
print('10 item with high event_type from user ')
print('----' * 8)

# Sort by event_type and limit to top 10, remove duplicates
top_item_user = (
    item_visited_by_user.sort_values(by='event_type', ascending=False)
    .head(10)  # Limiting to top 10 items
    .drop_duplicates(subset=['item_id'])  # Remove duplicates
    .item_id.values
)

item_df_rows = item_df[item_df['item_id'].isin(top_item_user)].drop_duplicates(subset=['item_id'])
for row in item_df_rows.itertuples():
    print(row.name, ':', row.category)

# Display top 10 item recommendation
print('Top 10 item recommendation')
print('----' * 8)

# Filter recommended items and remove duplicates
recommended_item = item_df[item_df['item_id'].isin(recommended_item_ids)].drop_duplicates(subset=['item_id'])

# Print item names and categories
for row in recommended_item.itertuples():
    print(row.name, ':', row.category)